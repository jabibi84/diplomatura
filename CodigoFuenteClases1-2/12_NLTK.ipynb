{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "para = \"\"\"Hello World. It's good to see you. Thanks for buying this book.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = nltk.sent_tokenize(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oración original: Hello World. It's good to see you. Thanks for buying this book.\n"
     ]
    }
   ],
   "source": [
    "print(\"Oración original: \" + para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oración 1: Hello World.\n"
     ]
    }
   ],
   "source": [
    "print(\"Oración 1: \" +  sent[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oración 2: It's good to see you.\n"
     ]
    }
   ],
   "source": [
    "print(\"Oración 2: \" +  sent[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oración 3: Thanks for buying this book.\n"
     ]
    }
   ],
   "source": [
    "print(\"Oración 3: \" +  sent[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'Hola amigo. Estoy bien.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_tokenizer = nltk.data.load('tokenizers/punkt/spanish.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_tokens = spanish_tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oración 1: Hola amigo.\n"
     ]
    }
   ],
   "source": [
    "print(\"Oración 1: \" +  spanish_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oración 2: Estoy bien.\n"
     ]
    }
   ],
   "source": [
    "print(\"Oración 2: \" +  spanish_tokens[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\"At eight o'clock on Thursday morning Arthur didn't feel very good.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['At', 'eight', \"o'clock\", 'on', 'Thursday', 'morning', 'Arthur', 'did', \"n't\", 'feel', 'very', 'good', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_exp_tokens = nltk.tokenize.regexp_tokenize(sentence, \"[\\w']+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['At', 'eight', \"o'clock\", 'on', 'Thursday', 'morning', 'Arthur', \"didn't\", 'feel', 'very', 'good']\n"
     ]
    }
   ],
   "source": [
    "print(reg_exp_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_exp_tokens = nltk.tokenize.regexp_tokenize(sentence, '\\s+', gaps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['At', 'eight', \"o'clock\", 'on', 'Thursday', 'morning', 'Arthur', \"didn't\", 'feel', 'very', 'good.']\n"
     ]
    }
   ],
   "source": [
    "print(reg_exp_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stops = set( nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stops.update(['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"Can't\", 'is', 'a', 'contraction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'for', 'this', 'didn', '!', 'the', 'until', 'weren', 'why', 'we', 'were', 'having', 'because', ']', 've', 'herself', 'she', 'once', 'as', 'should', 'her', ':', 'when', '[', 'down', 'between', 'yourselves', 'our', 'in', 'few', 'only', 'nor', 'that', 'from', 'yourself', 'will', 'aren', 'each', 'you', 'ourselves', 're', 'its', 'was', 'out', 'any', '}', 'him', 'not', 'm', 'most', 'or', 'haven', 'doing', 'it', 'here', ',', 'how', 'myself', 'what', ';', 'are', 'a', 'so', 'won', 'd', 'o', 'shouldn', 'and', 'against', 'wouldn', 'mustn', 'can', 'than', 'have', 'very', 'if', '\"', 'is', 'more', 'on', 'did', 'my', 'about', 'yours', '{', 'theirs', '?', 'now', 'itself', 'no', '.', 'being', 's', 'further', 'again', 'needn', 'over', 'too', 'during', 'don', 'under', ')', 'hasn', 'does', 'at', 'himself', 'has', 'there', 'both', 'just', 'hadn', 'their', 'y', 'some', 'be', 'll', 'had', 'into', 'they', 'am', 't', 'themselves', 'before', 'own', \"'\", 'by', 'isn', 'all', 'been', 'i', 'who', 'doesn', 'such', 'these', 'of', 'to', 'me', 'hers', 'which', 'but', 'above', 'shan', 'he', 'whom', 'mightn', 'off', 'those', 'with', 'then', 'ain', 'ma', 'other', 'couldn', 'while', 'where', 'below', 'up', 'his', 'an', 'after', 'do', 'them', 'wasn', 'same', 'ours', '(', 'through', 'your'}\n"
     ]
    }
   ],
   "source": [
    "print(english_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Stopwords: Can't\n",
      "No Stopwords: contraction\n"
     ]
    }
   ],
   "source": [
    "for i in words:\n",
    "    if i.lower() not in english_stops:\n",
    "        print(\"No Stopwords: \" + i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_stops = set( nltk.corpus.stopwords.words('spanish'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_stops.update(['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'con', 'mí', 'estaba', 'también', 'estuvieses', 'hubo', 'ha', 'sus', 'fueses', 'él', 'tendrían', 'tuvieseis', '!', 'míos', 'estas', 'tuviéramos', 'os', 'habría', 'hubieron', 'entre', 'fueron', 'tengas', 'tuviera', 'han', 'fueseis', 'vuestros', 'están', 'estés', ']', 'tengo', 'les', 'habíamos', 'quien', 'en', 'te', 'tengamos', 'mis', '?', 'otros', 'mía', 'seríamos', 'hubiese', 'poco', 'tendríais', 'teníais', 'tendrás', 'son', 'tuya', 'hubiera', 'tenemos', ':', '[', 'contra', 'serían', 'nuestros', 'habidos', 'somos', 'serías', 'mías', 'tengan', 'seamos', 'sin', 'vuestro', 'para', 'hayáis', 'habremos', 'habido', 'tienes', 'hubieseis', 'estando', 'tendremos', 'habrán', 'habrás', 'ese', 'le', 'será', 'teníamos', '{', 'fueras', '}', 'habíais', 'habéis', 'hubisteis', 'sea', 'sí', 'muchos', 'tus', 'tendríamos', 'teniendo', 'estad', 'estuviste', 'estén', 'tuviese', 'esté', 'habrían', 'hay', 'el', 'estás', 'estuvieras', 'hayas', 'tenidos', ',', 'hubierais', 'vosostras', 'estemos', 'siente', 'durante', 'algunos', 'del', 'yo', 'estada', 'tenían', 'estuvierais', ';', 'tened', 'tuvo', 'porque', 'fuéramos', 'como', 'hubiste', 'pero', 'fueran', 'a', 'ante', 'suya', 'tuyo', 'estuviésemos', 'todo', 'estábamos', 'una', 'tendrías', 'hubimos', 'o', 'habríamos', 'todos', 'soy', 'ellos', 'estuvisteis', 'habríais', 'tendréis', 'tuviésemos', '\"', 'tenida', 'estuvo', 'seas', 'habida', 'estaré', 'tenga', 'se', 'tenía', 'sentid', 'era', 'tendría', 'estarían', 'que', 'suyos', 'está', 'habían', 'fuesen', 'seríais', 'tuviste', 'tuvieses', 'es', 'tiene', 'e', 'tienen', 'estabas', 'sean', 'no', 'esto', 'sentidos', 'tuvieran', 'nada', 'otro', 'fuésemos', 'hubiéramos', 'fuisteis', '.', 'un', 'habrías', 'hubieses', 'estaréis', 'habréis', 'desde', 'estarán', 'estaría', 'tuvierais', 'estuviera', 'hasta', 'estoy', 'eres', 'haya', 'serán', 'tenéis', 'unos', 'seáis', 'estadas', 'ti', 'donde', 'hemos', 'qué', 'estará', 'serás', 'seremos', ')', 'hayamos', 'los', 'ya', 'tuvimos', 'este', 'esos', 'sintiendo', 'tu', 'sentida', 'sentidas', 'estuvimos', 'seré', 'éramos', 'estuviesen', 'vuestra', 'has', 'eras', 'estaremos', 'hayan', 'estuvieseis', 'y', 'las', 'por', 'seréis', 'tendrán', 'la', 'muy', 'mucho', 'estuviese', 'antes', 'sois', 'sería', 'fue', 'fuerais', 'fui', 'fuese', \"'\", 'vuestras', 'estaríais', 'nuestras', 'habrá', 'estarás', 'cual', 'estarías', 'hubieras', 'sentido', 'estéis', 'hubiesen', 'tú', 'hube', 'mi', 'estuvieron', 'estuve', 'habiendo', 'tendré', 'esas', 'estar', 'nosotros', 'mío', 'me', 'estabais', 'estados', 'vosostros', 'hubieran', 'tenido', 'habías', 'esta', 'de', 'ellas', 'nosotras', 'he', 'tuvisteis', 'eso', 'tuyas', 'tuyos', 'otras', 'tengáis', 'uno', 'tuvieron', 'estáis', 'hubiésemos', 'quienes', 'ella', 'otra', 'tanto', 'suyo', 'estaban', 'sobre', 'más', 'tenías', 'tuve', 'habré', 'estos', 'algo', 'eran', 'estado', 'estuvieran', 'estaríamos', 'algunas', 'había', 'fuiste', 'tendrá', 'habidas', 'fuera', 'al', 'ni', 'tenidas', 'nos', 'lo', 'tuviesen', 'esa', 'su', 'estuviéramos', 'nuestra', 'cuando', 'estamos', 'fuimos', '(', 'nuestro', 'erais', 'suyas', 'tuvieras'}\n"
     ]
    }
   ],
   "source": [
    "print(spanish_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"Cantar\", 'es', 'un', 'verbo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Stopwords: Cantar\n",
      "No Stopwords: verbo\n"
     ]
    }
   ],
   "source": [
    "for i in words:\n",
    "    if i.lower() not in spanish_stops:\n",
    "        print(\"No Stopwords: \" + i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "syns = nltk.corpus.wordnet.synsets('motorcar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car.n.01 (Es la primera definicion del sustantivo car)\n"
     ]
    }
   ],
   "source": [
    "print(syns[0].name() + \" (Es la primera definicion del sustantivo car)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('car.n.01')]\n"
     ]
    }
   ],
   "source": [
    "print(syns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "syns = nltk.corpus.wordnet.synset('car.n.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('ambulance.n.01'), Synset('beach_wagon.n.01'), Synset('bus.n.04'), Synset('cab.n.03'), Synset('compact.n.03'), Synset('convertible.n.01'), Synset('coupe.n.01'), Synset('cruiser.n.01'), Synset('electric.n.01'), Synset('gas_guzzler.n.01'), Synset('hardtop.n.01'), Synset('hatchback.n.01'), Synset('horseless_carriage.n.01'), Synset('hot_rod.n.01'), Synset('jeep.n.01'), Synset('limousine.n.01'), Synset('loaner.n.02'), Synset('minicar.n.01'), Synset('minivan.n.01'), Synset('model_t.n.01'), Synset('pace_car.n.01'), Synset('racer.n.02'), Synset('roadster.n.01'), Synset('sedan.n.01'), Synset('sport_utility.n.01'), Synset('sports_car.n.01'), Synset('stanley_steamer.n.01'), Synset('stock_car.n.01'), Synset('subcompact.n.01'), Synset('touring_car.n.01'), Synset('used-car.n.01')]\n"
     ]
    }
   ],
   "source": [
    "print(syns.hyponyms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "syns = nltk.corpus.wordnet.synsets('car')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['auto', 'automóvil', 'carro', 'coche', 'máquina', 'turismo', 'vehículo']\n"
     ]
    }
   ],
   "source": [
    "print(syns[0].lemma_names('spa'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('great.n.01'), Synset('great.s.01'), Synset('great.s.02'), Synset('great.s.03'), Synset('bang-up.s.01'), Synset('capital.s.03'), Synset('big.s.13')]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.corpus.wordnet.synsets('great'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total:       7\n"
     ]
    }
   ],
   "source": [
    "print(\"Total:       \" + str(len(nltk.corpus.wordnet.synsets('great'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sustantivos: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Sustantivos: \" + str(len(nltk.corpus.wordnet.synsets('great', pos='n'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjetivos:   6\n"
     ]
    }
   ],
   "source": [
    "print(\"Adjetivos:   \" + str(len(nltk.corpus.wordnet.synsets('great', pos='a'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.metrics import edit_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'hola' y 'bola': 1\n"
     ]
    }
   ],
   "source": [
    "print(\"'hola' y 'bola': \" + str(edit_distance('hola', 'bola')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
